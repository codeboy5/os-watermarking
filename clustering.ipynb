{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from tqdm import tqdm, trange\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "import numpy as np\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"meta-llama/Llama-2-7b-hf\"\n",
    "train = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "print(\"Using device: \", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_suffix = model_name.split(\"/\")[-1]\n",
    "\n",
    "y_dataset = torch.load(f\"data/y_dataset_{model_suffix}_filtered.pt\")\n",
    "X_dataset = torch.load(f\"data/X_dataset_{model_suffix}_filtered.pt\")\n",
    "\n",
    "token_frequencies = torch.load(f\"saved_models/openwebtext_token_freq.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes, counts = torch.unique(y_dataset, return_counts=True)\n",
    "\n",
    "large_classes = classes[counts > 50]\n",
    "\n",
    "# Remove small classes effectively\n",
    "mask = torch.isin(y_dataset, large_classes)\n",
    "\n",
    "X_dataset = X_dataset[mask]\n",
    "y_dataset = y_dataset[mask]\n",
    "\n",
    "total_classes = len(torch.unique(y_dataset))\n",
    "\n",
    "len(y_dataset), total_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = int(0.3 * len(y_dataset))\n",
    "unique_classes, class_counts = torch.unique(y_dataset, return_counts=True)\n",
    "num_classes = len(unique_classes)\n",
    "samples_per_class = max(1, S // num_classes)\n",
    "\n",
    "selected_indices = []\n",
    "for cls, count in zip(unique_classes, class_counts):\n",
    "    class_indices = (y_dataset == cls).nonzero(as_tuple=True)[0]\n",
    "\n",
    "    # Sample min(samples_per_class, available samples)\n",
    "    sampled_indices = class_indices[torch.randperm(\n",
    "        count)[:min(samples_per_class, count)]]\n",
    "    selected_indices.append(sampled_indices)\n",
    "\n",
    "# Concatenate once instead of multiple times\n",
    "selected_indices = torch.cat(selected_indices)\n",
    "\n",
    "# Shuffle to ensure randomness\n",
    "shuffled_indices = selected_indices[torch.randperm(len(selected_indices))[:S]]\n",
    "\n",
    "# Extract the final dataset\n",
    "X_dataset, y_dataset = X_dataset[shuffled_indices], y_dataset[shuffled_indices]\n",
    "\n",
    "len(y_dataset), len(torch.unique(y_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LearnableProjectionClustering(nn.Module):\n",
    "    def __init__(self, input_dim, num_clusters, reg_lambda=1e-2, balance_weight=0.1):\n",
    "        super().__init__()\n",
    "        self.num_clusters = num_clusters\n",
    "        self.reg_lambda = reg_lambda\n",
    "        self.balance_weight = balance_weight\n",
    "        self.total_clusters = num_clusters\n",
    "\n",
    "        self.projected_dim = self.total_clusters * 3\n",
    "\n",
    "        # Learnable projection matrix W (d' x d)\n",
    "        self.W = nn.Parameter(torch.randn(self.projected_dim, input_dim) * 0.1)\n",
    "\n",
    "        # Cluster centroids (d' x k)\n",
    "        self.centroids = nn.Parameter(\n",
    "            torch.randn(self.total_clusters, self.projected_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Project input data and normalize for cosine similarity\n",
    "        z = F.normalize(torch.matmul(x, self.W.T), dim=1)\n",
    "\n",
    "        # Compute cosine similarity with centroids\n",
    "        sim_matrix = torch.matmul(z, self.centroids.T)\n",
    "\n",
    "        # Assign clusters based on max similarity\n",
    "        cluster_assignments = torch.argmax(sim_matrix, dim=1)\n",
    "\n",
    "        return z, cluster_assignments\n",
    "\n",
    "    def loss(self, z, cluster_assignments):\n",
    "        # Compute clustering loss using cosine similarity\n",
    "        assigned_centroids = self.centroids[cluster_assignments]\n",
    "        cluster_loss = 1 - \\\n",
    "            F.cosine_similarity(z, assigned_centroids, dim=1).mean()\n",
    "\n",
    "        # Compute orthogonality loss (force centroids to be orthogonal)\n",
    "        centroid_norms = F.normalize(self.centroids, dim=1)\n",
    "        ortho_loss = torch.sum(torch.abs(torch.matmul(\n",
    "            centroid_norms, centroid_norms.T) - torch.eye(self.total_clusters).to(self.centroids.device)))\n",
    "\n",
    "        proj_variance_loss = torch.var(torch.norm(self.W, p=2, dim=-1))\n",
    "        # Cluster balancing loss (maximize entropy to encourage even distribution)\n",
    "        cluster_counts = torch.bincount(\n",
    "            cluster_assignments, minlength=self.total_clusters).float()\n",
    "        cluster_probs = cluster_counts / cluster_counts.sum()\n",
    "        # Avoid log(0)\n",
    "        balance_loss = -torch.sum(cluster_probs *\n",
    "                                  torch.log(cluster_probs + 1e-10))\n",
    "\n",
    "        # Weighted combination\n",
    "        return cluster_loss + ortho_loss + self.reg_lambda * proj_variance_loss + self.balance_weight * balance_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LearnableProjectionClustering2(nn.Module):\n",
    "    def __init__(self, input_dim, num_clusters, norm_lambda=1e-1, decorr_lambda=1e-3, balance_weight=0.1, tau_init=1.0, tau_min=0.05, tau_decay=0.99, ortho_lambda=1, projection_dim=200):\n",
    "\n",
    "        super().__init__()\n",
    "        self.num_clusters = num_clusters\n",
    "        self.norm_lambda = norm_lambda\n",
    "        self.decorr_lambda = decorr_lambda\n",
    "        self.balance_weight = balance_weight\n",
    "        self.ortho_lambda = ortho_lambda\n",
    "\n",
    "        # Projection matrix (learnable)\n",
    "        self.projected_dim = projection_dim\n",
    "        self.W = nn.Parameter(torch.randn(self.projected_dim, input_dim) * 0.1)\n",
    "\n",
    "        # Learnable cluster centroids\n",
    "        self.centroids = nn.Parameter(torch.randn(\n",
    "            self.num_clusters, self.projected_dim))\n",
    "\n",
    "        # Softmax temperature parameters\n",
    "        self.tau = tau_init\n",
    "        self.tau_min = tau_min\n",
    "        self.tau_decay = tau_decay\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass to assign clusters.\n",
    "        \"\"\"\n",
    "        # Project input features and normalize\n",
    "        z = F.normalize(torch.matmul(x, self.W.T), dim=1)\n",
    "\n",
    "        # Compute cosine similarity between projected points and centroids\n",
    "        sim_matrix = torch.matmul(z, self.centroids.T)\n",
    "\n",
    "        # Apply softmax with temperature for soft assignments\n",
    "        cluster_probs = F.softmax(sim_matrix / self.tau, dim=1)\n",
    "\n",
    "        return z, cluster_probs\n",
    "\n",
    "    def loss(self, z, cluster_probs):\n",
    "        \"\"\"\n",
    "        Computes clustering loss, orthogonality loss, projection regularization, and balance loss.\n",
    "        \"\"\"\n",
    "        # Soft cluster assignments to weighted centroids\n",
    "        assigned_centroids = torch.matmul(cluster_probs, self.centroids)\n",
    "\n",
    "        # Clustering loss (cosine similarity loss)\n",
    "        cluster_loss = 1 - \\\n",
    "            F.cosine_similarity(z, assigned_centroids, dim=1).mean()\n",
    "\n",
    "        # Orthogonality loss (encourages centroids to be well-separated)\n",
    "        centroid_norms = F.normalize(self.centroids, dim=1)\n",
    "        ortho_loss = torch.mean(torch.abs(torch.matmul(\n",
    "            centroid_norms, centroid_norms.T) - torch.eye(self.num_clusters).to(self.centroids.device)))\n",
    "\n",
    "        # Projection decorrelation loss (ensures projections are diverse)\n",
    "        # W_norm = F.normalize(self.W, dim=1)\n",
    "        # proj_corr_loss = torch.sum(torch.abs(torch.matmul(\n",
    "        #     W_norm, W_norm.T) - torch.eye(self.projected_dim).to(self.W.device)))\n",
    "\n",
    "        # Compute norms of projected vectors\n",
    "        z_norms = torch.norm(z, p=2, dim=1, keepdim=True)\n",
    "        # Compute weighted sum of norms per cluster\n",
    "        weighted_norms = cluster_probs.T @ z_norms\n",
    "        avg_norms = weighted_norms / \\\n",
    "            (cluster_probs.sum(dim=0, keepdim=True).T +\n",
    "             1e-10)  # Compute average norms\n",
    "        # Minimize variance of norms within each cluster\n",
    "        proj_norm_variance_loss = torch.mean(torch.var(avg_norms, dim=0))\n",
    "\n",
    "        # Balance loss (KL divergence to enforce even cluster distribution)\n",
    "        cluster_probs_mean = cluster_probs.mean(dim=0)  # Average across batch\n",
    "        uniform_target = torch.ones_like(\n",
    "            cluster_probs_mean) / self.num_clusters\n",
    "        balance_loss = F.kl_div((cluster_probs_mean + 1e-10).log(),\n",
    "                                uniform_target, reduction=\"batchmean\")\n",
    "\n",
    "        # Weighted combination\n",
    "        total_loss = (cluster_loss + self.ortho_lambda*ortho_loss +\n",
    "                      #   self.decorr_lambda * proj_corr_loss +\n",
    "                      self.norm_lambda * proj_norm_variance_loss +\n",
    "                      self.balance_weight * balance_loss)\n",
    "\n",
    "        return total_loss\n",
    "\n",
    "    def anneal_tau(self):\n",
    "        \"\"\"\n",
    "        Gradually reduces the softmax temperature to encourage hard clustering over time.\n",
    "        \"\"\"\n",
    "        self.tau = max(self.tau * self.tau_decay, self.tau_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stratified_split_optimized(X, y, val_split=0.2, random_seed=42):\n",
    "    \"\"\"\n",
    "    Efficient PyTorch implementation of stratified splitting.\n",
    "    \"\"\"\n",
    "    torch.manual_seed(random_seed)\n",
    "\n",
    "    # Get unique classes and their counts\n",
    "    unique_classes, class_counts = torch.unique(y, return_counts=True)\n",
    "\n",
    "    # Generate indices for entire dataset\n",
    "    indices = torch.arange(len(y))\n",
    "\n",
    "    # Storage for train and val indices\n",
    "    train_indices = []\n",
    "    val_indices = torch.zeros(int(len(y) * val_split), dtype=torch.long)\n",
    "\n",
    "    val_count = 0  # Track position for val_indices\n",
    "\n",
    "    for cls, count in zip(unique_classes, class_counts):\n",
    "        cls_mask = (y == cls)\n",
    "        cls_indices = indices[cls_mask]\n",
    "\n",
    "        # Shuffle the indices for this class\n",
    "        cls_indices = cls_indices[torch.randperm(count)]\n",
    "\n",
    "        # Compute number of validation samples\n",
    "        val_size = int(count * val_split)\n",
    "\n",
    "        # Store validation indices\n",
    "        val_indices[val_count:val_count + val_size] = cls_indices[:val_size]\n",
    "        val_count += val_size\n",
    "\n",
    "        # Store training indices (append to list)\n",
    "        train_indices.append(cls_indices[val_size:])\n",
    "\n",
    "    # Concatenate training indices\n",
    "    train_indices = torch.cat(train_indices)\n",
    "\n",
    "    # Shuffle train indices for randomness\n",
    "    train_indices = train_indices[torch.randperm(train_indices.shape[0])]\n",
    "\n",
    "    # Extract final train and validation sets\n",
    "    X_train, X_val = X[train_indices], X[val_indices]\n",
    "    y_train, y_val = y[train_indices], y[val_indices]\n",
    "\n",
    "    return X_train, X_val, y_train, y_val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_average_cosine_similarity(model, test_dataloader):\n",
    "    similarities = []\n",
    "    for batch in test_dataloader:\n",
    "        # Check if the data points are cosine similar to the centroids\n",
    "        z, cluster_assignments = model(batch[0].to(device))\n",
    "        cluster_assignments = torch.argmax(cluster_assignments, dim=1)\n",
    "        assigned_centroids = model.centroids[cluster_assignments]\n",
    "        similarity = F.cosine_similarity(z, assigned_centroids).mean()\n",
    "        similarities.append(similarity)\n",
    "\n",
    "\n",
    "    print(\"Average cosine similarity:\", torch.stack(similarities).mean().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cluster_count_variance(model, test_dataloader):\n",
    "    model.eval()\n",
    "    assignments = []\n",
    "    for batch in test_dataloader:\n",
    "        z, cluster_assignments = model(batch[0].to(device))\n",
    "        assignments.append(cluster_assignments)\n",
    "    \n",
    "    assignments = torch.cat(assignments, dim=0).mean(dim=0)\n",
    "    uniform_target = torch.ones_like(assignments) / model.num_clusters\n",
    "    balance_loss = F.kl_div((assignments + 1e-10).log(),\n",
    "                            uniform_target, reduction=\"batchmean\")\n",
    "    \n",
    "    print(\"Balance loss:\", balance_loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ortho_score(model):\n",
    "    centroids = model.centroids\n",
    "    centroid_norms = F.normalize(centroids, dim=1)\n",
    "    ortho_loss = torch.mean(torch.abs(torch.matmul(\n",
    "        centroid_norms, centroid_norms.T) - torch.eye(model.num_clusters).to(centroids.device)))\n",
    "    print(\"Orthogonality loss:\", ortho_loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "def train_model(model, train_dataloader, test_dataloader, epochs=100, lr=0.001, patience=5, topk=1000):\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    # Get top 1000 token frequences\n",
    "    topk_classes = torch.topk(token_frequencies, topk).indices\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0.0\n",
    "        model.train()\n",
    "        for batch in train_dataloader:\n",
    "            batch_data = batch[0].to(device)\n",
    "            optimizer.zero_grad()\n",
    "            z, cluster_assignments = model(batch_data)\n",
    "            loss = model.loss(z, cluster_assignments)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for batch in test_dataloader:\n",
    "                batch_data = batch[0].to(device)\n",
    "\n",
    "                z, cluster_assignments = model(batch_data)\n",
    "                loss = model.loss(z, cluster_assignments)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        model.anneal_tau()\n",
    "\n",
    "        avg_val_loss = val_loss / len(test_dataloader)\n",
    "\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        if patience_counter >= patience:\n",
    "            print(\n",
    "                f\"Early stopping at epoch {epoch} with validation loss {best_val_loss:.4f}\")\n",
    "            break\n",
    "\n",
    "        if epoch % 20 == 0:\n",
    "            print(\n",
    "                f\"Epoch {epoch}: Train Loss = {epoch_loss / len(train_dataloader):.4f}, Val Loss = {avg_val_loss:.4f}\")\n",
    "            compute_average_cosine_similarity(model, test_dataloader)\n",
    "            compute_cluster_count_variance(model, test_dataloader)\n",
    "            compute_ortho_score(model)\n",
    "            print(\"Tau:\", model.tau)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k = 4\n",
    "\n",
    "# # subsample from x_dataset and y_dataset\n",
    "# X_dataset = X_dataset[:400000]\n",
    "# y_dataset = y_dataset[:400000]\n",
    "\n",
    "# model = COPKMeans(n_clusters=k, y=y_dataset, device=\"cuda\", balance_factor=2)\n",
    "# model.fit(X_dataset)\n",
    "# labels = model.predict(X_dataset)\n",
    "\n",
    "# print(\"Davies Bouldin Index:\", davies_bouldin_index(X_dataset, labels))\n",
    "\n",
    "# cluster_counts = torch.bincount(labels, minlength=k)\n",
    "# cluster_variance = torch.var(cluster_counts.float()).item()\n",
    "\n",
    "# print(\"Cluster variance:\", cluster_variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = stratified_split_optimized(\n",
    "    X_dataset, y_dataset, val_split=0.3)\n",
    "\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "test_dataset = TensorDataset(X_val, y_val)\n",
    "\n",
    "batch_size = 8192\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "print(\"Train dataset size:\", len(train_dataset))\n",
    "print(\"Test dataset size:\", len(test_dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 4\n",
    "projection_dim = k + 8\n",
    "\n",
    "model = LearnableProjectionClustering2(\n",
    "    X_dataset.shape[1], k, balance_weight=0.3, ortho_lambda=5e-1, tau_init=1.8, tau_decay=0.995, norm_lambda=1e-1, decorr_lambda=0, projection_dim=projection_dim).to(device)\n",
    "\n",
    "if train:\n",
    "    train_model(model, train_dataloader, test_dataloader,\n",
    "                epochs=350, patience=350, lr=0.00005)\n",
    "    torch.save(model.state_dict(),\n",
    "               f'saved_models/projection_clustering_k3.pth')\n",
    "else:\n",
    "    model.load_state_dict(torch.load(\n",
    "        f'saved_models/projection_clustering.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if centroids are orthogonal\n",
    "centroids = model.centroids\n",
    "centroid_norms = F.normalize(centroids, dim=1)\n",
    "ortho_loss = torch.sum(torch.abs(torch.matmul(\n",
    "    centroid_norms, centroid_norms.T) - torch.eye(model.num_clusters).to(centroids.device)))\n",
    "print(\"Orthogonality loss:\", ortho_loss.item())\n",
    "\n",
    "\n",
    "similarities = []\n",
    "for batch in test_dataloader:\n",
    "    # Check if the data points are cosine similar to the centroids\n",
    "    z, cluster_assignments = model(batch[0].to(device))\n",
    "    cluster_assignments = torch.argmax(cluster_assignments, dim=1)\n",
    "    assigned_centroids = model.centroids[cluster_assignments]\n",
    "    similarity = F.cosine_similarity(z, assigned_centroids).mean()\n",
    "    similarities.append(similarity)\n",
    "\n",
    "\n",
    "print(\"Average cosine similarity:\", torch.stack(similarities).mean().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize accumulators\n",
    "total_sample_counts = torch.zeros(len(model.centroids), dtype=torch.long)\n",
    "\n",
    "# Accumulate counts\n",
    "for batch in train_dataloader:\n",
    "    z, cluster_assignments = model(batch[0].to(device))\n",
    "    cluster_assignments = torch.argmax(cluster_assignments, dim=1)\n",
    "    y_batch = batch[1].to(device)\n",
    "\n",
    "    for i in range(len(model.centroids)):\n",
    "        cluster_classes = y_batch[cluster_assignments == i]\n",
    "        unique_cluster_classes = torch.unique(cluster_classes)\n",
    "        sample_count = len(cluster_classes)\n",
    "        class_count = len(unique_cluster_classes)\n",
    "\n",
    "        total_sample_counts[i] += sample_count\n",
    "\n",
    "total = total_sample_counts.sum()\n",
    "# Print final accumulated counts\n",
    "for i in range(len(model.centroids)):\n",
    "    print(\n",
    "        f\"Cluster {i} - total sample count: {total_sample_counts[i]}, proportion: {total_sample_counts[i]/total}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_weight = torch.matmul(model.centroids, model.W)\n",
    "\n",
    "one_hots = torch.matmul(final_weight, X_dataset[:200000].to(device).T).T\n",
    "\n",
    "max_mean = torch.max(one_hots, dim=1).values.mean()\n",
    "\n",
    "final_weight = final_weight / max_mean\n",
    "\n",
    "one_hots = torch.matmul(final_weight, X_dataset[:200000].to(device).T).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hots[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "topk = torch.topk(one_hots, dim=1, k=k).values\n",
    "\n",
    "# Plot min and max values of one-hots\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.hist(topk[:, 0].detach().cpu().numpy(), bins=100, alpha=0.5, label=\"1\")\n",
    "plt.hist(topk[:, 1].detach().cpu().numpy(), bins=100, alpha=0.5, label=\"2\")\n",
    "plt.hist(topk[:, 2].detach().cpu().numpy(), bins=100, alpha=0.5, label=\"3\")\n",
    "plt.hist(topk[:, 3].detach().cpu().numpy(), bins=100, alpha=0.5, label=\"4\")\n",
    "plt.legend()\n",
    "plt.title(\"Min and Max values of one-hot encodings\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(topk.mean(dim=0))\n",
    "print(topk.std(dim=0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
