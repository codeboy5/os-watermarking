{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from collections import namedtuple\n",
    "from torch.utils.data import random_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
    "from mteb import MTEB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sts_tasks = [\n",
    "    \"STSBenchmark\",   # Standard semantic similarity benchmark\n",
    "    \"SICK-R\",         # Semantic relatedness from the SICK dataset\n",
    "    \"STS12\",\n",
    "    \"STS13\",\n",
    "    \"STS14\",\n",
    "    \"STS15\",\n",
    "    \"STS16\",\n",
    "]\n",
    "\n",
    "evaluation = MTEB(tasks=sts_tasks)\n",
    "\n",
    "\n",
    "def eval(eval_model):\n",
    "    results = evaluation.run(eval_model, output_folder=None)\n",
    "    score = [result.scores['test'][0]['main_score']\n",
    "                     for result in results]\n",
    "    tasks = [result.task_name for result in results]\n",
    "    # Print list of tasks as comma-separated values\n",
    "    print(\", \".join(tasks))\n",
    "    # Print list of scores as comma-separated values    \n",
    "    print(\", \".join([f\"{score:.4f}\" for score in score]))\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"meta-llama/Llama-2-7b-hf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class LLaMAEmbeddingModel:\n",
    "    def __init__(self, model, tokenizer, normalize_embeddings=True, mean_pooling=True, layer_idx = -1, ignore_bos_token = False, batch_size=8, W_aug=torch.nn.Identity()):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.model.eval()\n",
    "        # Remove the second half of decoder layers\n",
    "        # self.model.model.layers = self.model.model.layers[:len(self.model.model.layers)//2]\n",
    "        self.model.lm_head = torch.nn.Identity()\n",
    "        self.normalize_embeddings = normalize_embeddings\n",
    "        self.mean_pooling = mean_pooling\n",
    "        self.batch_size = batch_size\n",
    "        self.layer_idx = layer_idx\n",
    "        self.ignore_bos_token = ignore_bos_token\n",
    "        self.W_aug = W_aug.cuda()\n",
    "\n",
    "    def encode(self, sentences, **kwargs):\n",
    "        all_embeddings = []\n",
    "        for i in range(0, len(sentences), self.batch_size):\n",
    "            batch = sentences[i:i+self.batch_size]\n",
    "            inputs = self.tokenizer(\n",
    "                batch, return_tensors='pt', padding=True, truncation=True).to(self.model.device)\n",
    "            with torch.no_grad():\n",
    "                output = self.model(**inputs, output_hidden_states=True, return_dict=True)\n",
    "                outputs = output.hidden_states[self.layer_idx]\n",
    "                # Mean pooling\n",
    "                if self.mean_pooling:\n",
    "                    attention_mask = inputs['attention_mask']\n",
    "                    embeddings = outputs * attention_mask.unsqueeze(-1)\n",
    "                    if self.ignore_bos_token:\n",
    "                        # Ignore the first token (BOS)\n",
    "                        embeddings = embeddings[:, 1:, :]\n",
    "\n",
    "                    embeddings = embeddings.mean(1)\n",
    "                else:\n",
    "                    # get last token embedding\n",
    "                    embeddings = outputs[:, -1, :]\n",
    "                    # Apply ridge regression projection\n",
    "                    embeddings = self.W_aug(embeddings)\n",
    "\n",
    "                if self.normalize_embeddings:\n",
    "                    embeddings = torch.nn.functional.normalize(\n",
    "                        embeddings, p=2, dim=1)\n",
    "                all_embeddings.append(embeddings.cpu().float().numpy())\n",
    "        return np.vstack(all_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'meta-llama/Llama-2-7b-hf'\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name,  device_map=\"auto\", torch_dtype=torch.bfloat16)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "eval_model = LLaMAEmbeddingModel(\n",
    "    model, tokenizer, normalize_embeddings=False, mean_pooling=False)\n",
    "print(eval(eval_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_model = LLaMAEmbeddingModel(\n",
    "    model, tokenizer, normalize_embeddings=False, mean_pooling=True)\n",
    "print(eval(eval_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_layers = len(model.model.layers)\n",
    "\n",
    "eval_model = LLaMAEmbeddingModel(\n",
    "    model, tokenizer, normalize_embeddings=False, mean_pooling=True, ignore_bos_token=False, layer_idx=19)\n",
    "print(eval(eval_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "print(eval(eval_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_model = SentenceTransformer(\"all-mpnet-base-v2\")\n",
    "print(eval(eval_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer(\"WhereIsAI/UAE-Large-V1\")\n",
    "print(eval(model))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
