\begin{thebibliography}{14}
\providecommand{\natexlab}[1]{#1}

\bibitem[{Aaronson(2023)}]{aaronson2023reform}
Scott Aaronson. 2023.
\newblock {20 - 'Reform' AI Alignment with Scott Aaronson}.
\newblock
  \url{https://axrp.net/episode/2023/04/11/episode-20-reform-ai-alignment-scott-aaronson.html}.
\newblock AXRP - The AI X-risk Research Podcast.

\bibitem[{Ajith et~al.(2024)Ajith, Singh, and
  Pruthi}]{ajith-etal-2024-downstream}
Anirudh Ajith, Sameer Singh, and Danish Pruthi. 2024.
\newblock \href {https://doi.org/10.18653/v1/2024.findings-emnlp.821}
  {Downstream trade-offs of a family of text watermarks}.
\newblock In \emph{Findings of the Association for Computational Linguistics:
  EMNLP 2024}, pages 14039--14053, Miami, Florida, USA. Association for
  Computational Linguistics.

\bibitem[{Block et~al.(2025)Block, Sekhari, and Rakhlin}]{block2025gaussmark}
Adam Block, Ayush Sekhari, and Alexander Rakhlin. 2025.
\newblock Gaussmark: A practical approach for structural watermarking of
  language models.
\newblock \emph{arXiv preprint arXiv:2501.13941}.

\bibitem[{Christ et~al.(2024)Christ, Gunn, Malkin, and
  Raykova}]{christ2024provably}
Miranda Christ, Sam Gunn, Tal Malkin, and Mariana Raykova. 2024.
\newblock Provably robust watermarks for open-source language models.
\newblock \emph{arXiv preprint arXiv:2410.18861}.

\bibitem[{Elhassan et~al.(2025)Elhassan, Ajroldi, Orvieto, and
  Geiping}]{elhassan2025can}
Fay Elhassan, Niccol{\`o} Ajroldi, Antonio Orvieto, and Jonas Geiping. 2025.
\newblock Can you finetune your binoculars? embedding text watermarks into the
  weights of large language models.
\newblock \emph{arXiv preprint arXiv:2504.06446}.

\bibitem[{Gloaguen et~al.(2025)Gloaguen, Jovanović, Staab, and
  Vechev}]{gloaguen2025towards}
Thibaud Gloaguen, Nikola Jovanović, Robin Staab, and Martin Vechev. 2025.
\newblock Towards watermarking of open-source llms.
\newblock \emph{arXiv preprint arXiv:2502.10525}.

\bibitem[{Gu et~al.(2023)Gu, Li, Liang, and Hashimoto}]{gu2023learnability}
Chenchen Gu, Xiang~Lisa Li, Percy Liang, and Tatsunori Hashimoto. 2023.
\newblock On the learnability of watermarks for language models.
\newblock \emph{arXiv preprint arXiv:2312.04469}.

\bibitem[{Jiang et~al.(2023)Jiang, Sablayrolles, Mensch, Bamford, Chaplot,
  de~las Casas, Bressand, Lengyel, Lample, Saulnier, Lavaud, Lachaux, Stock,
  Scao, Lavril, Wang, Lacroix, and Sayed}]{jiang2023mistral7b}
Albert~Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford,
  Devendra~Singh Chaplot, Diego de~las Casas, Florian Bressand, Gianna Lengyel,
  Guillaume Lample, Lucile Saulnier, Lélio~Renard Lavaud, Marie-Anne Lachaux,
  Pierre Stock, Teven~Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix,
  and William~El Sayed. 2023.
\newblock \href {https://arxiv.org/abs/2310.06825} {Mistral 7b}.
\newblock \emph{Preprint}, arXiv:2310.06825.

\bibitem[{Kirchenbauer et~al.(2023)Kirchenbauer, Geiping, Wen, Katz, Miers, and
  Goldstein}]{kirchenbauer2023watermark}
John Kirchenbauer, Jonas Geiping, Yuxin Wen, Jonathan Katz, Ian Miers, and Tom
  Goldstein. 2023.
\newblock A watermark for large language models.
\newblock In \emph{International Conference on Machine Learning}, pages
  17061--17084. PMLR.

\bibitem[{Kuditipudi et~al.(2023)Kuditipudi, Thickstun, Hashimoto, and
  Liang}]{kuditipudi2023robust}
Rohith Kuditipudi, John Thickstun, Tatsunori Hashimoto, and Percy Liang. 2023.
\newblock Robust distortion-free watermarks for language models.
\newblock \emph{arXiv preprint arXiv:2307.15593}.

\bibitem[{Liu and Bu(2024)}]{liu2024adaptive}
Yepeng Liu and Yuheng Bu. 2024.
\newblock Adaptive text watermark for large language models.
\newblock \emph{arXiv preprint arXiv:2401.13927}.

\bibitem[{Qwen et~al.(2025)Qwen, :, Yang, Yang, Zhang, Hui, Zheng, Yu, Li, Liu,
  Huang, Wei, Lin, Yang, Tu, Zhang, Yang, Yang, Zhou, Lin, Dang, Lu, Bao, Yang,
  Yu, Li, Xue, Zhang, Zhu, Men, Lin, Li, Tang, Xia, Ren, Ren, Fan, Su, Zhang,
  Wan, Liu, Cui, Zhang, and Qiu}]{qwen2025qwen25technicalreport}
Qwen, :, An~Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo~Zheng, Bowen Yu,
  Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang,
  Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, and 25
  others. 2025.
\newblock \href {https://arxiv.org/abs/2412.15115} {Qwen2.5 technical report}.
\newblock \emph{Preprint}, arXiv:2412.15115.

\bibitem[{Touvron et~al.(2023)Touvron, Martin, Stone, Albert, Almahairi,
  Babaei, Bashlykov, Batra, Bhargava, Bhosale et~al.}]{touvron2023llama}
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine
  Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale,
  and 1 others. 2023.
\newblock Llama 2: Open foundation and fine-tuned chat models.
\newblock \emph{arXiv preprint arXiv:2307.09288}.

\bibitem[{Xu et~al.(2024)Xu, Yao, and
  Liu}]{xu2024learningwatermarkllmgeneratedtext}
Xiaojun Xu, Yuanshun Yao, and Yang Liu. 2024.
\newblock \href {https://arxiv.org/abs/2403.10553} {Learning to watermark
  llm-generated text via reinforcement learning}.
\newblock \emph{Preprint}, arXiv:2403.10553.

\end{thebibliography}
