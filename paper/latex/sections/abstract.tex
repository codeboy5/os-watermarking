\begin{abstract}
With the rise of human-like text generation by large language models (LLMs), reliably attributing machine-generated content has become increasingly important for combating misinformation, ensuring content provenance, and enforcing responsible AI usage. Watermarking, which embeds identifiable statistical signals in the generated text, offers a promising solution. However, traditional watermarking approaches assume exclusive control over the generation pipeline, typically embedding signals by modifying the text sampling process during inference. This assumption breaks down in the open-source setting, where models are fully accessible and users have unrestricted control over how text is generated. Existing open-weight watermarking methods often require costly tuning strategies, such as reinforcement learning, distillation from watermarked text, or supervised fine-tuning. In this work, we propose a method that directly modifies the unembedding layer through a structured perturbation conditioned on the modelâ€™s hidden states, in order to steer generation toward watermarked outputs. Our approach requires no retraining and integrates much faster than tuning-based methods, without sacrificing text fluency or watermark detectability. We conduct comprehensive evaluations on real-world, popular benchmarks and demonstrate that our watermarked model maintains strong performance. In addition, we show robustness to paraphrasing attacks and resilience to post-hoc model finetuning, establishing the practicality and effectiveness of our approach for watermarking open-source language models.
\end{abstract}