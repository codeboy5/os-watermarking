\section{Introduction}

Watermarking techniques aim to embed imperceptible signals into model-generated text to enable provenance detection and mitigate misuse. While most existing approaches operate at decoding time by modifying next-token probabilities \citep{kirchenbauer2023watermark, aaronson2023reform, kuditipudi2023robust, liu2024adaptive}, these methods can be easily bypassed in open-weight models, where users have full control over the generation process. This motivates a shift toward techniques that embed the watermarking logic directly into the model's weights to produce detectable artifacts in the output distribution.

Existing approaches typically rely on distillation from watermarked teachers, finetuning, or reinforcement learning to steer generation toward watermarked outputs \citep{gu2023learnability, xu2024learningwatermarkllmgeneratedtext, elhassan2025can}. These tuning-based methods are computationally expensive, involve complex training pipelines, and struggle to maintain watermark detectability under low-distortion configurations.

Edit-based watermarking offers a lightweight alternative by modifying select model weights without retraining. \citet{christ2024provably} introduces a Gaussian watermark via an added output bias, but this non-standard component can be trivially removed. \textsc{GaussMark} \cite{block2025gaussmark} avoids architectural changes by perturbing existing weights and detecting watermarks via a gradient-based z-score, but suffers from weak detection signals and requires both forward and backward passes, limiting practicality.

In this work, we introduce a new \emph{edit-based watermarking framework} that modifies the unembedding layer weights—the parameters that project final hidden states to output vocabulary logits—by adding structured perturbations. These perturbations induce dynamic logit biases during generation that subtly influence token sampling in a detectable way. Unlike prior logit-based watermarking strategies such as \citet{kirchenbauer2023watermark} and \citet{liu2024adaptive}, which bias logit values during decoding, our approach directly embeds the biasing logic into the unembedding layer's weights. This design ensures that the watermark cannot be easily removed or circumvented, even when users have full control over the decoding process or inference code. Moreover, our framework is general-purpose: it supports multiple watermarking schemes that produce different biasing patterns, all of which are compatible with a unified detection method.

To detect the watermark, we apply a simple test: the \textbf{average log-likelihood ratio (LLR)} per token between the watermarked model and a reference model. This test captures subtle but consistent shifts in token probabilities introduced by the watermark. Crucially, it only requires forward passes and works across multiple watermarking schemes within our framework, provided they produce detectable logit perturbations.

We evaluate our method on three popular open-source LLMs, Llama-2-7b \citet{touvron2023llama}, Mistral-7B-v0.3 \citet{jiang2023mistral7b} and Qwen2.5-3B \citet{qwen2025qwen25technicalreport} under a range of conditions. Our results demonstrate that the proposed approach consistently achieves:

\begin{enumerate}

    \item Strong watermark detectability using the average LLR test

    \item Minimal impact on generation quality

\end{enumerate}

The rest of the paper is organized as follows: Section~\ref{sec:background_related} provides background on language model watermarking, with a focus on the unique challenges posed by open-source settings. Section~\ref{sec:methodology} introduces a general unembedding-layer watermarking framework, along with two concrete schemes and a corresponding statistical detection method. Section~\ref{sec:experiments} presents empirical evaluations of watermark detectability and text quality, including downstream task performance. Section~\ref{sec:paraphasing} examines robustness to paraphrasing attacks, and Section~\ref{sec:finetuning} investigates the durability of the watermark under model fine-tuning. Section~\ref{sec:conclusion} summarizes our findings and outlines directions for future work.