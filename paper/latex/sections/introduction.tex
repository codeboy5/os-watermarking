\section{Introduction}

As open-source large language models (LLMs) become increasingly capable and widely available, ensuring reliable attribution of AI-generated content has become more urgent. Watermarking—embedding imperceptible statistical signals into generated text—has emerged as a promising technique to trace provenance and deter misuse. Traditional watermarking methods, however, rely on generation-time interventions that modify the decoding process to guide token selection \cite{kirchenbauer2023watermark}. These techniques are fundamentally incompatible with open-weight settings, where end-users have full access to and control over the model internals, including the sampling strategy.

To address this limitation, recent work embeds watermarks directly into a model's weights so it naturally generates watermarked text. Methods include distillation, which fine-tunes the model on outputs from a decoder-based watermarked teacher \cite{gu2023learnability}; reinforcement learning, using a separate detector model to assign rewards that guide watermark insertion \cite{xu2024learningwatermarkllmgeneratedtext}; and LoRA-based fine-tuning \cite{hu2022lora}, in which lightweight modules and a detector are trained jointly to encourage both watermark generation and detection \cite{elhassan2025can}. While effective, these approaches are often computationally expensive.

Edit-based watermarking methods offer a lightweight alternative by directly modifying select model weights without any retraining. For instance, \textsc{Provably Robust} \cite{christ2024provably} introduces a Gaussian watermark into a newly added bias vector in the output layer. While effective, this added component is non-standard and can be easily stripped without affecting model behavior. \textsc{Gaussmark} \cite{block2025gaussmark}, in contrast, avoids architectural changes by perturbing existing weight subsets and detects watermarks using a z-score computed from the dot product between the perturbation and the gradient of log-likelihood. However, a limitation of \textsc{Gaussmark} is that the detection signal is weak and the method requires both a forward pass and a partial backward pass, making it less practical for deployment.

In this work, we introduce a new \emph{edit-based watermarking method} that modifies the unembedding layer weights—the parameters that project final hidden states to output vocabulary logits—by adding a structured perturbation. This perturbation introduces a dynamic logit bias at every generation step that influences token sampling in a detectable way. Our approach is similar to prior logit-based watermarking strategies such as \citet{kirchenbauer2023watermark} and \citet{liu2024adaptive}, but differs in that the biasing logic is embedded directly into the model's weights rather than applied during decoding. As a result, the watermark cannot be easily removed, even when users have full control over the decoding logic.

To detect the watermark, we use a simple and scalable metric: the \textbf{average log-likelihood ratio (LLR)} per token between the watermarked model and the unmodified reference model. This test captures subtle but consistent shifts in token probabilities introduced by the watermark. Importantly, the method requires only forward passes and is compatible with any strategy that modifies the logits during generation. 

We evaluate our method on two popular open-source LLMs, Llama-2-7b and Mistral-7B-v0.3, under a range of conditions. Our results demonstrate that the proposed approach consistently achieves:

\begin{enumerate}

    \item Strong watermark detectability using the average LLR test

    \item Minimal impact on text quality

    \item Robustness to paraphrasing and fine-tuning.

\end{enumerate}

The rest of the paper is organized as follows. Section~\ref{sec:related} reviews prior watermarking strategies and introduces open-weight challenges \cite{gloaguen2025towards}. Section~\ref{sec:method} defines our unembedding-layer watermarking formulation and detection method. Section~\ref{sec:experiments} presents evaluations of watermark detection, text quality, and robustness under realistic attacks.