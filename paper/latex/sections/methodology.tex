\section{Methodology}
\label{sec:methodology}

This section introduces our watermarking method, which embeds structured perturbations into the unembedding matrix of a causal language model. We first define key notations, then describe the watermark construction and detection strategy, followed by our primary instantiation.

\subsection{Preliminaries}

Let a causal language model process a sequence of tokens drawn from a vocabulary \( \mathcal{V} = \{w_1, w_2, \dots, w_{|\mathcal{V}|}\} \). Let \( x_i \in \mathcal{V} \) denote the \( i \)-th token in the sequence, and let \( x_{<t} = (x_1, \dots, x_{t-1}) \) denote the prefix up to timestep \( t \).

The model computes a hidden representation \( h_t = f(x_{<t}) \in \mathbb{R}^d \), where \( f: \mathcal{V}^* \rightarrow \mathbb{R}^d \) is the model's internal encoding function. The logit vector \( v_t \in \mathbb{R}^{|\mathcal{V}|} \) for predicting the next token is given by:

\begin{equation}
    v_t = U h_t,
\end{equation}

where \( U \in \mathbb{R}^{|\mathcal{V}| \times d} \) is the unembedding matrix.

\subsection{Watermarking via Unembedding Perturbation}

We define a watermark by modifying the unembedding matrix with a structured perturbation:
\begin{equation}
    \tilde{U} = U + \Delta W,
\end{equation}
where \( \Delta W \in \mathbb{R}^{|V| \times d} \) is the watermarking matrix. This induces a modified logit vector:
\begin{equation}
    \tilde{v}_t = \tilde{U} h_t = v_t + \Delta W h_t,
\end{equation}
effectively applying a hidden-state-dependent logit bias during generation.

This general formulation enables a broad class of watermarking strategies. The structure of \( \Delta W \) governs the efficacy of the watermark. To be effective, \( \Delta W \) should produce logit biases that are:
\begin{itemize}
    \item \textbf{Detectable:} The logit biases should induce statistically identifiable changes in output probabilities.
    \item \textbf{Stealthy:} Perturbations should preserve fluency and perplexity of generated text, requiring logit biases to be in a reasonable range.
    \item \textbf{Diverse:} The logit biases should vary across contexts and timesteps, making it harder to reverse-engineer.
\end{itemize}
We describe two constructions of \( \Delta W \) that satisfy these criteria.


\subsubsection{Green List Biasing}

This method draws inspiration from the watermarking approach of \citet{kirchenbauer2023watermark}, which boosts the logits of tokens belonging to a pseudorandomly selected \emph{green list}. At each timestep, the green list is generated using a pseudorandom function (PRF) that depends on a secret seed and the preceding \( n \) tokens in the prefix. This ensures that the token biases vary in a deterministic, structured, and context-sensitive manner.

To encode this behavior directly into the model weights, we express the perturbation matrix \( \Delta W \) as the product of two matrices:
\begin{equation}
    \Delta W = G H,
\end{equation}
where:

\begin{itemize}
    \item \( G \in \mathbb{R}^{|\mathcal{V}| \times C} \) contains \( C \) watermarking lists represented as row vectors. For each pseudo-class \( c \in \{1, \dots, C\} \), the corresponding row \( H_c \in \mathbb{R}^{|\mathcal{V}|} \) is defined as:
    \[
    (H_c)_i = 
    \begin{cases}
    \delta & \text{if } i \in \mathcal{G}_c \\
    0 & \text{otherwise}
    \end{cases}
    \]
    where \( \delta > 0 \) is a fixed scalar that determines the strength of the watermark signal, and \( \mathcal{G}_c \subseteq \{1, \dots, |\mathcal{V}|\} \) is the green list for class \( c \), constructed using a pseudorandom function:
    \[
    \mathcal{G}_c = \left\{ i \in \{1, \dots, |\mathcal{V}|\} \;\middle|\; \mathrm{PRF}(\text{seed}, c, i) < \gamma \right\}.
    \]
    Here, \(\mathrm{PRF}(\cdot)\) is a keyed hash function seeded with a secret key, and \(\gamma \in (0,1)\) is a fixed threshold that determines the fraction of green-listed indices.

    \item \( H \in \mathbb{R}^{C \times d} \) maps each token's hidden state to a soft pseudo-class one-hot selector. To construct \( H \), we proceed as follows:
    \begin{enumerate}
        \item Run the base model on a large unlabeled corpus (e.g., OpenWebText) and collect hidden states from the last layer before the unembedding layer.
            \item Apply \( k \)-means clustering to these hidden states, assigning each to one of \( C \) pseudo-classes.
            \item Solve a ridge regression problem to map each hidden state \( h \in \mathbb{R}^d \) to a soft one-hot selector \( s \in \mathbb{R}^C \), approximating the discrete cluster assignments.
    \end{enumerate}
\end{itemize}

This construction ensures that hidden states in similar regions of representation space receive similar perturbations. Although the selectors in \( G \) are soft, they closely approximate discrete assignments and enable reliable watermark detection. The structured factorization of \( \Delta W \) into \( G \) and \( H \) allows multiple green list rules to be encoded in a single matrix perturbation.

\subsubsection{Gaussian Random Projection}

In this variant, the watermark is embedded by applying a fixed Gaussian perturbation to the unembedding matrix. Specifically, the perturbation matrix \( \Delta W \in \mathbb{R}^{|\mathcal{V}| \times d} \) is initialized as:
\begin{equation}
    \Delta W_{ij} \sim \mathcal{N}(0, 1).
\end{equation}

During generation, this produces a dynamic, input-dependent logit bias:
\[
\Delta \ell_t = \Delta W h_t \in \mathbb{R}^{|\mathcal{V}|},
\]
which is added to the model’s original logits.

Unlike the green list biasing approach, which relies on discrete vocabulary partitions, this method introduces a continuous watermarking signal by projecting the hidden state through a random Gaussian matrix. Although the hidden state \( h_t \in \mathbb{R}^d \) is not necessarily Gaussian, the Central Limit Theorem ensures that the projected logits \( [\Delta \ell_t]_i = \langle \Delta W_i, h_t \rangle \) are approximately Gaussian-distributed when \( d \) is large:

\begin{theorem}[CLT for Logit Bias Projection]
    Let \( h_t \in \mathbb{R}^d \) be fixed, and let each row \( \Delta W_i \in \mathbb{R}^d \) be drawn independently from \( \mathcal{N}(0, I_d) \). Then the scalar projection
    \[
        [\Delta W h_t]_i = \langle \Delta W_i, h_t \rangle = \sum_{j=1}^d \Delta W_{ij} \cdot h_{tj}
    \]
    converges in distribution to \( \mathcal{N}(0, \|h_t\|^2) \) as \( d \to \infty \).
\end{theorem}

Consequently, the logit bias vector \( \Delta \ell_t \) is approximately distributed as \( \mathcal{N}(0, \|h_t\|^2 I) \), meaning that, at each timestep, about half of the tokens are boosted while the other half are suppressed. This introduces a subtle but statistically detectable watermark signal without degrading generation quality.

To ensure consistent perturbation magnitude across timesteps and model scales, we normalize the matrix using the expected norm of a hidden state vector and introduce a scaling hyperparameter \( \delta > 0 \) to control watermark strength:
\begin{equation}
    \Delta W \leftarrow \frac{\delta}{\mathbb{E}[\|h_t\|]} \cdot \Delta W.
\end{equation}

\subsection{Detection via Likelihood Ratio Test}

To detect the watermark, we use a length-normalized log-likelihood ratio (LLR) test between the watermarked and reference models:
\begin{equation} \label{eq:llr}
    \text{LLR}(x) = \frac{1}{T} \sum_{t=1}^{T} \log \frac{p_{\text{wm}}(x_t \mid x_{<t})}{p_{\text{ref}}(x_t \mid x_{<t})},
\end{equation}
where \( p_{\text{wm}} \) and \( p_{\text{ref}} \) denote the softmax probabilities computed using the watermarked and original unembedding matrices, respectively.

These probabilities are defined as:
\begin{align}
    p_{\text{ref}}(x_t \mid x_{<t}) & =
    \frac{\exp(U_{x_t} h_t)}{\sum_{j=1}^{|V|} \exp(U_j h_t)}, \\
    p_{\text{wm}}(x_t \mid x_{<t})  & =
    \frac{\exp(\tilde{U}_{x_t} h_t)}{\sum_{j=1}^{|V|} \exp(\tilde{U}_j h_t)},
\end{align}
where \( U \in \mathbb{R}^{|V| \times d} \) is the original unembedding matrix, \( \tilde{U} = U + \Delta W \) is the watermarked version, and \( h_t \in \mathbb{R}^d \) is the hidden state at timestep \( t \).

Substituting these into Equation~\ref{eq:llr}, we obtain:
\begin{equation}
    \begin{aligned}
        \text{LLR}(x)
         & = \frac{1}{T} \sum_{t=1}^{T} \Big(
        (\tilde{U}_{x_t} - U_{x_t}) h_t                                       \\
         & \quad - \log \frac{\sum_j e^{\tilde{U}_j h_t}}{\sum_j e^{U_j h_t}}
        \Big)
    \end{aligned}
\end{equation}

This expression decomposes the LLR into two interpretable terms:
\begin{itemize}
    \item A \textit{token-level logit shift} term, \( (\tilde{U}_{x_t} - U_{x_t}) h_t \), which directly reflects the effect of watermarking on the predicted token's logit.
    \item A \textit{partition function ratio} term, which captures the normalization difference across the entire vocabulary.
\end{itemize}

\paragraph{Detection Protocol.}
The model developer releases the model with the watermarked unembedding matrix \( \tilde{U} = U + \Delta W \), while keeping the original matrix \( U \)—and by extension the perturbation \( \Delta W \)—secret. To verify whether a given piece of text \( x \) was generated by the watermarked model, the developer computes the LLR between the public (watermarked) model and the hidden reference model using Equation~\ref{eq:llr}. A significantly positive LLR indicates the presence of the watermark signal.

This detection process can be kept private or deployed via a walled API, allowing third parties to query the watermark status of text without exposing the reference model or the perturbation matrix.


% TODO:
% - Describe expected value of LLR under the null hypothesis (no watermark) and the alternative hypothesis (watermark present).
% - Discuss why a LLR is better than logit difference. Show rigorousness justification.