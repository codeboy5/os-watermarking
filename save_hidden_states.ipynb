{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x767008286990>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import einops\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from rich import print as rprint\n",
    "from typing import Dict, List\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, LogitsProcessorList\n",
    "\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_count = torch.load(\"saved_models/openwebtext_token_freq.pt\")\n",
    "\n",
    "token_probs = token_count / token_count.sum()\n",
    "\n",
    "probability_mass = token_probs.topk(5000).values.sum()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"meta-llama/Llama-2-7b-hf\"\n",
    "#model_name = \"EleutherAI/pythia-1b\"\n",
    "#model_name = \"/assets/models/meta-llama-3.1-8b\"\n",
    "load_model = True\n",
    "compute_freq = False\n",
    "total = 3000\n",
    "save_interval = 100\n",
    "start = 0\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\")\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "if load_model:\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name,\n",
    "                                                 device_map=\"auto\")\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device:  cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "print(\"Using device: \", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<0x0A>',\n",
       " '▁the',\n",
       " ',',\n",
       " '.',\n",
       " '▁to',\n",
       " '▁of',\n",
       " '▁a',\n",
       " '▁and',\n",
       " '▁in',\n",
       " '▁',\n",
       " 's',\n",
       " '0',\n",
       " '-',\n",
       " '1',\n",
       " '▁that',\n",
       " '’',\n",
       " '▁is',\n",
       " '▁for',\n",
       " '▁on',\n",
       " '2',\n",
       " \"'\",\n",
       " '▁with',\n",
       " '▁was',\n",
       " '<s>',\n",
       " '▁it',\n",
       " 'ing',\n",
       " '▁as',\n",
       " '▁at',\n",
       " '▁be',\n",
       " '▁by',\n",
       " '▁I',\n",
       " 'ed',\n",
       " '▁from',\n",
       " '▁are',\n",
       " ':',\n",
       " '▁has',\n",
       " '▁have',\n",
       " '▁The',\n",
       " '▁(',\n",
       " 'The',\n",
       " '▁an',\n",
       " '5',\n",
       " '3',\n",
       " '▁this',\n",
       " '▁you',\n",
       " '▁his',\n",
       " '▁he',\n",
       " '▁“',\n",
       " '9',\n",
       " '4',\n",
       " '▁not',\n",
       " 't',\n",
       " '▁said',\n",
       " '▁will',\n",
       " 'ers',\n",
       " '6',\n",
       " ')',\n",
       " '▁or',\n",
       " '▁their',\n",
       " '▁who',\n",
       " '▁but',\n",
       " '7',\n",
       " '▁we',\n",
       " '▁they',\n",
       " '▁\"',\n",
       " '▁been',\n",
       " '8',\n",
       " '▁about',\n",
       " '▁out',\n",
       " '▁one',\n",
       " '▁more',\n",
       " '▁can',\n",
       " 'es',\n",
       " 'y',\n",
       " '▁which',\n",
       " '\"',\n",
       " '▁were',\n",
       " '▁up',\n",
       " '▁had',\n",
       " '▁all',\n",
       " '▁its',\n",
       " '▁new',\n",
       " '▁A',\n",
       " '▁over',\n",
       " '▁would',\n",
       " '▁after',\n",
       " 'er',\n",
       " 'ly',\n",
       " '▁first',\n",
       " 'in',\n",
       " '▁when',\n",
       " 'S',\n",
       " '▁people',\n",
       " '▁than',\n",
       " '▁T',\n",
       " '▁into',\n",
       " '?',\n",
       " '▁what',\n",
       " '▁time',\n",
       " '▁her',\n",
       " 'ic',\n",
       " '”',\n",
       " '/',\n",
       " '▁some',\n",
       " '▁so',\n",
       " '▁also',\n",
       " 'al',\n",
       " '▁two',\n",
       " '▁C',\n",
       " 'm',\n",
       " 'I',\n",
       " '▁like',\n",
       " 'A',\n",
       " '▁just',\n",
       " '▁there',\n",
       " '▁your',\n",
       " '▁our',\n",
       " '▁It',\n",
       " '▁—',\n",
       " 'day',\n",
       " '▁other',\n",
       " '▁year',\n",
       " '▁S',\n",
       " '▁years',\n",
       " '▁my',\n",
       " '▁D',\n",
       " 'ation',\n",
       " '“',\n",
       " 'an',\n",
       " 're',\n",
       " '▁M',\n",
       " ',”',\n",
       " '▁do',\n",
       " '▁them',\n",
       " '▁last',\n",
       " '▁G',\n",
       " '▁B',\n",
       " 'or',\n",
       " '▁In',\n",
       " '▁no',\n",
       " '▁if',\n",
       " '▁most',\n",
       " '▁now',\n",
       " 'on',\n",
       " '▁F',\n",
       " '▁H',\n",
       " 'a',\n",
       " '▁un',\n",
       " '▁P',\n",
       " '▁R',\n",
       " '▁$',\n",
       " '▁–',\n",
       " '▁only',\n",
       " '▁b',\n",
       " '.”',\n",
       " '▁how',\n",
       " 've',\n",
       " '▁U',\n",
       " '▁could',\n",
       " '▁back',\n",
       " '▁she',\n",
       " 'ated',\n",
       " '▁N',\n",
       " 'ist',\n",
       " '▁week',\n",
       " 'ate',\n",
       " ',\"',\n",
       " '▁-',\n",
       " 'is',\n",
       " '▁get',\n",
       " 'C',\n",
       " '▁L',\n",
       " '▁off',\n",
       " '▁W',\n",
       " '▁K',\n",
       " '▁Trump',\n",
       " '▁being',\n",
       " 'd',\n",
       " '▁He',\n",
       " '▁many',\n",
       " '▁him',\n",
       " 'it',\n",
       " 'B',\n",
       " '▁make',\n",
       " '▁world',\n",
       " '!',\n",
       " 'In',\n",
       " '▁re',\n",
       " 'ations',\n",
       " 'T',\n",
       " 'i',\n",
       " '▁me',\n",
       " '▁p',\n",
       " '▁before',\n",
       " '▁any',\n",
       " 'D',\n",
       " '▁because',\n",
       " 'th',\n",
       " '▁made',\n",
       " '▁where',\n",
       " 'ar',\n",
       " '▁even',\n",
       " 'en',\n",
       " '▁This',\n",
       " '.\"',\n",
       " '▁New',\n",
       " '▁against',\n",
       " 'M',\n",
       " 'ive',\n",
       " 'o',\n",
       " 'ity',\n",
       " '▁game',\n",
       " 'ary',\n",
       " '▁E',\n",
       " '▁way',\n",
       " 'It',\n",
       " 'h',\n",
       " '▁But',\n",
       " '▁three',\n",
       " 'ating',\n",
       " '▁down',\n",
       " 'ies',\n",
       " 'e',\n",
       " '▁t',\n",
       " '▁those',\n",
       " '▁these',\n",
       " 'able',\n",
       " '▁through',\n",
       " '▁work',\n",
       " '▁while',\n",
       " '▁under',\n",
       " '▁well',\n",
       " 'am',\n",
       " '▁told',\n",
       " '▁team',\n",
       " 'ans',\n",
       " '▁much',\n",
       " '▁don',\n",
       " '▁government',\n",
       " 'le',\n",
       " '▁We',\n",
       " '▁c',\n",
       " '▁during',\n",
       " 'el',\n",
       " ';',\n",
       " 'et',\n",
       " 'ian',\n",
       " '▁sp',\n",
       " '▁very',\n",
       " 'ists',\n",
       " '▁know',\n",
       " '▁see',\n",
       " '▁then',\n",
       " 'ors',\n",
       " '▁around',\n",
       " '▁may',\n",
       " 'est',\n",
       " '▁since',\n",
       " 'man',\n",
       " 'H',\n",
       " '▁state',\n",
       " 'ings',\n",
       " '▁r',\n",
       " '▁take',\n",
       " '▁w',\n",
       " '▁says',\n",
       " '▁between',\n",
       " '▁use',\n",
       " '▁s',\n",
       " '▁say',\n",
       " '▁man',\n",
       " 'P',\n",
       " '▁st',\n",
       " '▁day',\n",
       " 'as',\n",
       " 'b',\n",
       " '▁going',\n",
       " 'F',\n",
       " 'ia',\n",
       " 'ine',\n",
       " 'R',\n",
       " '▁V',\n",
       " '▁part',\n",
       " '▁us',\n",
       " 'We',\n",
       " '▁right',\n",
       " '▁own',\n",
       " '▁J',\n",
       " '▁long',\n",
       " 'at',\n",
       " '▁police',\n",
       " '▁should',\n",
       " '▁still',\n",
       " 'N',\n",
       " '▁here',\n",
       " '),',\n",
       " '—',\n",
       " '▁found',\n",
       " '▁such',\n",
       " '▁sh',\n",
       " 'ant',\n",
       " 'year',\n",
       " '▁end',\n",
       " 'end',\n",
       " 'G',\n",
       " 'ment',\n",
       " 'O',\n",
       " '▁Th',\n",
       " '▁f',\n",
       " 'c',\n",
       " 'us',\n",
       " '▁public',\n",
       " '▁g',\n",
       " 'W',\n",
       " '▁law',\n",
       " 'L',\n",
       " '▁home',\n",
       " '▁O',\n",
       " '▁president',\n",
       " '▁season',\n",
       " '▁used',\n",
       " '▁good',\n",
       " 'ial',\n",
       " '▁d',\n",
       " ').',\n",
       " '▁And',\n",
       " '▁President',\n",
       " '▁few',\n",
       " 'als',\n",
       " '▁million',\n",
       " '▁did',\n",
       " '▁de',\n",
       " '▁think',\n",
       " '▁go',\n",
       " 'ie',\n",
       " 'n',\n",
       " 'id',\n",
       " '▁want',\n",
       " 'old',\n",
       " ']',\n",
       " '▁show',\n",
       " 'f',\n",
       " 'and',\n",
       " 'K',\n",
       " '▁h',\n",
       " 'ous',\n",
       " 'ally',\n",
       " '▁same',\n",
       " 'ad',\n",
       " '▁news',\n",
       " '▁both',\n",
       " '▁next',\n",
       " '▁country',\n",
       " 'g',\n",
       " 'E',\n",
       " 'ent',\n",
       " 'ch',\n",
       " '▁according',\n",
       " 'ues',\n",
       " '▁company',\n",
       " 'This',\n",
       " 'ism',\n",
       " 'w',\n",
       " 'r',\n",
       " 'z',\n",
       " 'led',\n",
       " 'ama',\n",
       " 'k',\n",
       " '▁need',\n",
       " '▁called',\n",
       " 'ates',\n",
       " '▁including',\n",
       " '▁m',\n",
       " 'p',\n",
       " 'ion',\n",
       " '▁high',\n",
       " '▁United',\n",
       " '▁set',\n",
       " 'ative',\n",
       " 'ance',\n",
       " 'com',\n",
       " 'ur',\n",
       " '▁play',\n",
       " '▁group',\n",
       " 'J',\n",
       " '▁US',\n",
       " 'll',\n",
       " '▁dis',\n",
       " '▁every',\n",
       " '▁l',\n",
       " '▁best',\n",
       " '▁Sh',\n",
       " '▁today',\n",
       " '▁city',\n",
       " '▁American',\n",
       " '▁Ob',\n",
       " '▁life',\n",
       " '▁--',\n",
       " 'But',\n",
       " '▁head',\n",
       " 'ens',\n",
       " 'urs',\n",
       " '▁another',\n",
       " 'age',\n",
       " '▁really',\n",
       " '▁former',\n",
       " 'ot',\n",
       " \"▁'\",\n",
       " '▁come',\n",
       " '▁help',\n",
       " '▁four',\n",
       " '▁They',\n",
       " '▁Ch',\n",
       " '▁night',\n",
       " '▁number',\n",
       " 'ay',\n",
       " '▁‘',\n",
       " '▁got',\n",
       " 'l',\n",
       " '▁war',\n",
       " '▁far',\n",
       " '▁campaign',\n",
       " 'il',\n",
       " 'ics',\n",
       " 'ters',\n",
       " '▁system',\n",
       " '▁look',\n",
       " '▁support',\n",
       " 'ats',\n",
       " '▁report',\n",
       " '▁days',\n",
       " '▁place',\n",
       " '▁find',\n",
       " '▁second',\n",
       " '▁You',\n",
       " '▁left',\n",
       " '▁ch',\n",
       " 'ut',\n",
       " '▁top',\n",
       " 'u',\n",
       " '▁past',\n",
       " '▁reported',\n",
       " '▁per',\n",
       " 'ions',\n",
       " '▁things',\n",
       " '▁each',\n",
       " '▁[',\n",
       " '▁am',\n",
       " '▁month',\n",
       " '▁video',\n",
       " '▁something',\n",
       " '▁percent',\n",
       " '▁St',\n",
       " '▁As',\n",
       " '%',\n",
       " 'ill',\n",
       " '▁lot',\n",
       " '▁Y',\n",
       " 'um',\n",
       " 'ure',\n",
       " '▁might',\n",
       " 'ling',\n",
       " '▁car',\n",
       " '▁too',\n",
       " 'ins',\n",
       " '▁little',\n",
       " '▁For',\n",
       " '▁York',\n",
       " 'os',\n",
       " '▁research',\n",
       " 'ants',\n",
       " 'nes',\n",
       " '▁never',\n",
       " 'co',\n",
       " '▁Cl',\n",
       " 'st',\n",
       " 'ile',\n",
       " '▁ago',\n",
       " '▁business',\n",
       " '▁media',\n",
       " '▁post',\n",
       " 'ash',\n",
       " '▁Re',\n",
       " '▁House',\n",
       " '▁already',\n",
       " 'ical',\n",
       " '▁months',\n",
       " '▁using',\n",
       " '▁different',\n",
       " '▁without',\n",
       " '▁women',\n",
       " 'He',\n",
       " '▁big',\n",
       " '▁series',\n",
       " '▁came',\n",
       " 'ets',\n",
       " '▁Monday',\n",
       " '▁health',\n",
       " '▁took',\n",
       " '▁data']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(token_probs.topk(500).indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset with `[X,y]` with `X=h(t)` and `y=token(t)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to get the feature vector before the final unembedding layer\n",
    "model.lm_head = torch.nn.Identity() # llama\n",
    "model.embed_out = torch.nn.Identity() #pythia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_length(example):\n",
    "    return len(tokenizer(example[\"text\"])['input_ids']) >= 256\n",
    "\n",
    "def encode_text(example):\n",
    "    prompt = tokenizer(\n",
    "        example[\"text\"], truncation=True, max_length=256, return_tensors=\"pt\" )\n",
    "    example['input_ids'] = prompt['input_ids']\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"Skylion007/openwebtext\",\n",
    "                       split=\"train\", streaming=True, trust_remote_code=True)\n",
    "dataset = dataset.filter(filter_length)\n",
    "dataset = dataset.map(encode_text, batched=True)\n",
    "dataset = dataset.shuffle(seed=42)\n",
    "subset = dataset.skip(start*batch_size)\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if compute_freq:\n",
    "    # Get token count and probability\n",
    "    count_dataloader = torch.utils.data.DataLoader(dataset, batch_size=512)\n",
    "    token_count = torch.zeros(tokenizer.vocab_size).to(device)\n",
    "    pbar = tqdm(total=1000)\n",
    "\n",
    "    for batch in count_dataloader:\n",
    "        tokens = batch[\"input_ids\"].to(device).flatten()\n",
    "        token_count.scatter_add_(0, tokens, torch.ones_like(tokens, dtype=torch.float32))\n",
    "        pbar.update(1)\n",
    "        if pbar.n >= 1000:\n",
    "            break  \n",
    "\n",
    "    pbar.close()\n",
    "    torch.save(token_count, \"saved_models/openwebtext_token_freq.pt\")\n",
    "else:\n",
    "    token_count = torch.load(\"saved_models/openwebtext_token_freq.pt\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_probs = token_count / token_count.sum()\n",
    "prob_threshold = torch.quantile(token_probs, 0.8).to(device)  # Filter the top 20% most frequent tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_suffix = model_name.split(\"/\")[-1]\n",
    "\n",
    "# Define save directory and file paths\n",
    "save_dir = \"data\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "x_save_path = os.path.join(save_dir, f\"X_dataset_{model_suffix}\")\n",
    "y_save_path = os.path.join(save_dir, f\"y_dataset_{model_suffix}\")\n",
    "\n",
    "# Initialize lists for accumulating data\n",
    "X_all = []\n",
    "y_all = []\n",
    "\n",
    "# Define save interval (e.g., every 10 batches)\n",
    "file_index = start//save_interval\n",
    "\n",
    "pbar = tqdm(total=total)\n",
    "i = 0\n",
    "alpha = 1\n",
    "\n",
    "for batch in dataloader:\n",
    "    with torch.no_grad():\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        output = model(input_ids).logits\n",
    "\n",
    "        outputs = einops.rearrange(\n",
    "            output, \"batch pos hdim -> (batch pos) hdim\")\n",
    "        inputs = einops.rearrange(input_ids, \"batch pos -> (batch pos)\")\n",
    "\n",
    "        # Sample down based on token probability\n",
    "        # Fetch probabilities for sampled tokens\n",
    "        token_probs_batch = token_probs[inputs]\n",
    "        rand_vals = torch.rand_like(\n",
    "            token_probs_batch)  # Generate random values\n",
    "\n",
    "        # Compute adjusted drop probability for high-probability tokens\n",
    "        adjusted_drop_prob = alpha * token_probs_batch  # Reduce drop probability\n",
    "\n",
    "        # Masking logic:\n",
    "        mask = (token_probs_batch < prob_threshold) | (\n",
    "            rand_vals > adjusted_drop_prob)\n",
    "\n",
    "        X_all.append(outputs[mask].detach().cpu())  \n",
    "        y_all.append(inputs[mask].detach().cpu())\n",
    "\n",
    "    # Save periodically to avoid memory overload\n",
    "    if (i + 1) % save_interval == 0:\n",
    "        if X_all:\n",
    "            # Convert list of tensors to a single tensor\n",
    "            X_vec = torch.cat(X_all, dim=0)\n",
    "            y_vec = torch.cat(y_all, dim=0)\n",
    "\n",
    "            # Save updated tensors\n",
    "            torch.save(X_vec, f'{x_save_path}_{file_index}.pt')\n",
    "            torch.save(y_vec, f'{y_save_path}_{file_index}.pt')\n",
    "            file_index += 1\n",
    "            # Clear memory after saving\n",
    "            X_all = []\n",
    "            y_all = []\n",
    "\n",
    "    pbar.update(1)\n",
    "    i += 1\n",
    "    if i >= total:\n",
    "        break\n",
    "\n",
    "# Final save for remaining data\n",
    "if X_all:\n",
    "    X_vec = torch.cat(X_all, dim=0)\n",
    "    y_vec = torch.cat(y_all, dim=0)\n",
    "\n",
    "    torch.save(X_vec, f\"{x_save_path}_{file_index}.pt\")\n",
    "    torch.save(y_vec, f\"{y_save_path}_{file_index}.pt\")\n",
    "\n",
    "# Close the progress bar\n",
    "pbar.close()\n",
    "print(\n",
    "    f\"Saved hidden states to {x_save_path} and token inputs to {y_save_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
