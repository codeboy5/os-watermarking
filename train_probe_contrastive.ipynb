{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7dc00c1829b0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import argparse\n",
    "import random\n",
    "import math\n",
    "import json\n",
    "import time \n",
    "import einops\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from rich import print as rprint\n",
    "from typing import Dict, List\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, LogitsProcessorList\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"meta-llama/Llama-2-7b-hf\"\n",
    "#model_name = \"EleutherAI/pythia-1b\"\n",
    "#model_name = \"/assets/models/meta-llama-3.1-8b\"\n",
    "load_model = False\n",
    "train = True\n",
    "load_linear_model = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\")\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = None\n",
    "if load_model:\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name,\n",
    "                                                 device_map=\"auto\")\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    model.eval()\n",
    "    # extract the embed_token layer and purge the model from memory\n",
    "    w = model.model.embed_tokens.weight.data.detach().clone()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device:  cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "print(\"Using device: \", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset with `[X,y]` with `X=h(t)` and `y=token(t)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_suffix = model_name.split(\"/\")[-1]\n",
    "\n",
    "X_dataset = torch.load(f\"data/X_dataset_{model_suffix}_top1000.pt\")\n",
    "y_dataset = torch.load(f\"data/y_dataset_{model_suffix}_top1000.pt\")\n",
    "total_classes = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_dataset_np = X_dataset.numpy()\n",
    "y_dataset_np = y_dataset.numpy()\n",
    "\n",
    "# Initialize StratifiedShuffleSplit\n",
    "splitter = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "\n",
    "# Perform stratified split\n",
    "for train_index, val_index in splitter.split(X_dataset_np, y_dataset_np):\n",
    "    # Get train and validation data\n",
    "    X_train, X_val = X_dataset[train_index], X_dataset[val_index]\n",
    "    y_train, y_val = y_dataset[train_index], y_dataset[val_index]\n",
    "\n",
    "X_train = torch.tensor(X_train)\n",
    "X_val = torch.tensor(X_val)\n",
    "y_train = torch.tensor(y_train)\n",
    "y_val = torch.tensor(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean = X_train.mean()\n",
    "# std = X_train.std()\n",
    "\n",
    "# X_train = (X_train - mean) / std\n",
    "# y_train = (y_train - mean) / std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train Dataset Size: \", X_train.shape)\n",
    "print(\"Val Dataset Size: \", X_val.shape)\n",
    "hidden_dim = X_dataset.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "test_dataset = TensorDataset(X_val, y_val)\n",
    "\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=4096, shuffle=True)\n",
    "val_dataloader = DataLoader(test_dataset, batch_size=4096)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear probe for current token classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Derive soft labels from inverse transformations of token embeddings using the psuedoinverse of the embedding layer matrix\n",
    "- Train linear model without bias using MSE loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiplier = 3\n",
    "\n",
    "class LinearProbe(nn.Module):\n",
    "    def __init__(self, hdim=hidden_dim, n_classes=len(tokenizer)):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(hdim, n_classes, bias=False)\n",
    "\n",
    "    def forward(self, X):\n",
    "        x = self.fc1(X)\n",
    "        return x\n",
    "\n",
    "\n",
    "def cosine_orthogonality_loss(Y, labels, lambda_intra=1.0, lambda_inter=1.0, lambda_reg_intra=1.0, lambda_reg_inter=1.0):\n",
    "\n",
    "    labels_expanded = labels.unsqueeze(1)\n",
    "    same_class_mask = labels_expanded == labels_expanded.T\n",
    "    diff_class_mask = labels_expanded != labels_expanded.T\n",
    "\n",
    "    Y_normalized = F.normalize(Y, p=2, dim=1, eps=1e-6)\n",
    "\n",
    "    cosine_similarity = torch.matmul(\n",
    "        Y_normalized, Y_normalized.T)\n",
    "\n",
    "    inner_product = torch.matmul(Y, Y.T)\n",
    "\n",
    "    # Intra-class loss: encourage cosine similarity close to 1\n",
    "    intra_class_sims = cosine_similarity[same_class_mask]\n",
    "    # intra_class_var = torch.var(intra_class_ip)\n",
    "    intra_class_loss = torch.mean(1 - intra_class_sims)\n",
    "\n",
    "    # Inter-class loss: encourage orthogonality (cosine similarity close to 0)\n",
    "    inter_class_sims = cosine_similarity[diff_class_mask]\n",
    "    inter_class_loss = torch.mean(inter_class_sims ** 2)\n",
    "\n",
    "    intra_class_ip = torch.abs(inner_product[same_class_mask])\n",
    "    inter_class_ip = torch.abs(inner_product[diff_class_mask])\n",
    "\n",
    "    intra_class_variance = torch.var(intra_class_ip)\n",
    "    inter_class_variance = torch.var(inter_class_ip)\n",
    "    regularization_term = lambda_reg_inter*intra_class_variance + \\\n",
    "        lambda_reg_inter*inter_class_variance\n",
    "    # Final loss with proper weighting\n",
    "    loss = lambda_intra * intra_class_loss + lambda_inter * \\\n",
    "        inter_class_loss + regularization_term\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "linear_nn = LinearProbe(hidden_dim, total_classes*multiplier)\n",
    "linear_nn.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 500\n",
    "# Implement StepLR to decrease the learning rate by a factor of 0.5 every 50 epochs\n",
    "learning_rate = 1e-4\n",
    "\n",
    "optimizer = torch.optim.AdamW(linear_nn.parameters(), lr=learning_rate)\n",
    "\n",
    "steplr = torch.optim.lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.5)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate():\n",
    "    # Validation Epoch\n",
    "    linear_nn.eval()\n",
    "    total_inter_class_ip = 0\n",
    "    total_intra_class_ip = 0\n",
    "    total_inter_class_ip_sqr = 0\n",
    "    total_intra_class_ip_sqr = 0\n",
    "    total_inter_class_sim = 0\n",
    "    total_intra_class_sim = 0\n",
    "    num_inter_pairs = 0\n",
    "    num_intra_pairs = 0\n",
    "    correct = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(val_dataloader):\n",
    "            X = batch[0].to(device)\n",
    "            y = batch[1].to(device)\n",
    "            logits = linear_nn(X)\n",
    "            logits_normalized = F.normalize(logits, p=2, dim=1, eps=1e-6)\n",
    "\n",
    "            inner_product = torch.matmul(logits, logits.T)\n",
    "            cosine_similarity = torch.matmul(\n",
    "                logits_normalized, logits_normalized.T)\n",
    "\n",
    "            y_batch = y.unsqueeze(1)\n",
    "            same_class_mask = (y_batch == y_batch.T)\n",
    "            diff_class_mask = (y_batch != y_batch.T)\n",
    "\n",
    "            # Compute statistics with proper masking\n",
    "            intra_class_sim = cosine_similarity[same_class_mask]\n",
    "            intra_class_ip = inner_product[same_class_mask]\n",
    "\n",
    "            inter_class_sim = cosine_similarity[diff_class_mask]\n",
    "            inter_class_ip = inner_product[diff_class_mask]\n",
    "\n",
    "            # Correct calculation: Compare average intra-class similarity to average inter-class similarity\n",
    "            avg_intra_class_ip = intra_class_ip.mean() if intra_class_ip.numel() > 0 else 0\n",
    "            avg_inter_class_ip = inter_class_ip.mean() if inter_class_ip.numel() > 0 else 0\n",
    "            correct += avg_intra_class_ip > avg_inter_class_ip\n",
    "\n",
    "            # Sum for mean/variance computation\n",
    "            total_inter_class_ip += inter_class_ip.sum()\n",
    "            total_inter_class_ip_sqr += inter_class_ip.pow(2).sum()\n",
    "\n",
    "            total_intra_class_ip += intra_class_ip.sum()\n",
    "            total_intra_class_ip_sqr += intra_class_ip.pow(2).sum()\n",
    "\n",
    "            total_inter_class_sim += inter_class_sim.sum()\n",
    "            total_intra_class_sim += intra_class_sim.sum()\n",
    "\n",
    "            num_intra_pairs += same_class_mask.sum()\n",
    "            num_inter_pairs += diff_class_mask.sum()\n",
    "\n",
    "    # Compute means safely (avoid division by zero)\n",
    "    num_inter_pairs = max(num_inter_pairs, 1)  # Prevent zero division\n",
    "    num_intra_pairs = max(num_intra_pairs, 1)\n",
    "\n",
    "    mean_inter_class_sim = total_inter_class_sim / num_inter_pairs\n",
    "    mean_intra_class_sim = total_intra_class_sim / num_intra_pairs\n",
    "\n",
    "    mean_intra_class_ip = total_intra_class_ip / num_intra_pairs\n",
    "    var_intra_class_ip = total_intra_class_ip_sqr / \\\n",
    "        num_intra_pairs - mean_intra_class_ip**2\n",
    "\n",
    "    mean_inter_class_ip = total_inter_class_ip / num_inter_pairs\n",
    "    var_inter_class_ip = total_inter_class_ip_sqr / \\\n",
    "        num_inter_pairs - mean_inter_class_ip**2\n",
    "\n",
    "    # Correct Accuracy Calculation\n",
    "    accuracy = correct / len(val_dataloader)  # Average across batches\n",
    "\n",
    "    print(\"Validation Accuracy: \", accuracy.item())\n",
    "    print(f\"Inter-class similarity: {mean_inter_class_sim.item()}\")\n",
    "    print(f\"Intra-class similarity: {mean_intra_class_sim.item()}\")\n",
    "    print(\n",
    "        f\"Inter-class inner product std dev: {torch.sqrt(var_inter_class_ip).item()}\")\n",
    "    print(\n",
    "        f\"Intra-class inner product std dev: {torch.sqrt(var_intra_class_ip).item()}\")\n",
    "    \n",
    "    return mean_inter_class_ip.item(), mean_intra_class_ip.item(), var_inter_class_ip.item(), var_intra_class_ip.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train and load_linear_model:\n",
    "    linear_nn.load_state_dict(torch.load(\n",
    "        f\"saved_models/linear_probe_contrastive_c{total_classes}.pth\"))\n",
    "    print(\"Loaded Linear Model\")\n",
    "    validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "val_accs = []\n",
    "\n",
    "if train:\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        # Training epoch\n",
    "        linear_nn.train()\n",
    "        epoch_train_loss = 0.0\n",
    "\n",
    "        for i, batch in enumerate(train_dataloader):\n",
    "            X = batch[0].to(device)\n",
    "            y = batch[1].to(device)\n",
    "            outputs = linear_nn(X)\n",
    "            loss = cosine_orthogonality_loss(\n",
    "                outputs, y, lambda_intra=400, lambda_inter=400, lambda_reg_inter=0.1, lambda_reg_intra=0.6)  # Compute loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            epoch_train_loss += loss.item()\n",
    "            \n",
    "        steplr.step()\n",
    "        avg_train_loss = epoch_train_loss / len(train_dataloader)\n",
    "        train_losses.append(avg_train_loss)\n",
    "        if epoch % 5 == 0:\n",
    "            print(f\"Epoch: {epoch} with train loss:\", avg_train_loss)\n",
    "            val_accs.append(validate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train:\n",
    "    mean_inter_class_ip, mean_intra_class_ip, var_inter_class_ip, var_intra_class_ip = zip(*\n",
    "                                                                                           val_accs)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(1, num_epochs + 1), train_losses, label='Training Loss')\n",
    "    plt.plot(range(1, num_epochs//5 + 1),\n",
    "             mean_inter_class_ip, label='Inter-class IP')\n",
    "    plt.plot(range(1, num_epochs//5 + 1),\n",
    "             mean_intra_class_ip, label='Intra-class IP')\n",
    "    plt.plot(range(1, num_epochs//5 + 1), var_inter_class_ip,\n",
    "             label='Inter-class IP Variance')\n",
    "    plt.plot(range(1, num_epochs//5 + 1), var_intra_class_ip,\n",
    "             label='Intra-class IP Variance')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    print(\"Mean Inter-class IP: \",\n",
    "          mean_inter_class_ip[-1], mean_inter_class_ip[-2])\n",
    "    print(\"Mean Intra-class IP: \",\n",
    "          mean_intra_class_ip[-1], mean_intra_class_ip[-2])\n",
    "    print(\"Variance Inter-class IP: \",\n",
    "          var_inter_class_ip[-1], var_inter_class_ip[-2])\n",
    "    print(\"Variance Intra-class IP: \",\n",
    "          var_intra_class_ip[-1], var_intra_class_ip[-2])\n",
    "    print(\"Train Loss: \", train_losses[-1], train_losses[-2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine class means with the original embeddings and compute the inner product similarity matrix that should transform hidden states to one-hot encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "    linear_nn.eval()\n",
    "\n",
    "    unique_classes = torch.unique(y_dataset)\n",
    "    class_means = torch.zeros(len(tokenizer), multiplier*total_classes)\n",
    "    cosine_similarity_scores = []\n",
    "\n",
    "    for class_idx in tqdm(unique_classes):\n",
    "        # Find all samples of the same class\n",
    "        class_mask = y_dataset == class_idx\n",
    "        X_class = X_dataset[class_mask]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = linear_nn(X_class.to(device))\n",
    "\n",
    "        # Normalize logits\n",
    "        logits = F.normalize(logits, p=2, dim=1, eps=1e-6)\n",
    "\n",
    "        # Compute cosine similarity matrix\n",
    "        cosine_similarity = torch.matmul(logits, logits.T)\n",
    "        # get average cosine similarity\n",
    "        cosine_similarity_scores.append(cosine_similarity.mean().item())\n",
    "\n",
    "        class_means[class_idx] = logits.mean(dim=0)\n",
    "\n",
    "    weights = linear_nn.fc1.weight.data\n",
    "    final_weights = torch.matmul(class_means.to(weights.device), weights)\n",
    "    linear_nn_final = LinearProbe(hidden_dim, len(tokenizer)).to(device)\n",
    "    with torch.no_grad():\n",
    "        linear_nn_final.fc1.weight.copy_(final_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot histogram of cosine similarity scores\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(cosine_similarity_scores, bins=50)\n",
    "plt.xlabel('Cosine Similarity')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Cosine Similarity Scores')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "correct = 0\n",
    "# Let's calculate the accuracy on the validation set\n",
    "for i, batch in enumerate(val_dataloader):\n",
    "    with torch.no_grad():\n",
    "        X = batch[0].to(device)\n",
    "        y = batch[1].to(device)\n",
    "        outputs = linear_nn_final(X)\n",
    "        labels = outputs.argmax(dim=1)\n",
    "\n",
    "        correct += (labels == y).sum()\n",
    "\n",
    "print(\"Validation Accuracy: \", correct.item()/X_val.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train:\n",
    "    linear_nn.eval()\n",
    "\n",
    "    unique_classes = torch.unique(y_val)\n",
    "\n",
    "    max_values = []\n",
    "    min_values = []\n",
    "\n",
    "    for i, batch in enumerate(val_dataloader):\n",
    "        X = batch[0].to(device)\n",
    "        y = batch[1].to(device)\n",
    "        with torch.no_grad():\n",
    "            logits = linear_nn_final(X)\n",
    "        max_values.append(logits.max(dim=1).values)\n",
    "        min_values.append(logits.min(dim=1).values)\n",
    "        \n",
    "    max_values = torch.cat(max_values)\n",
    "    min_values = torch.cat(min_values)\n",
    "\n",
    "    print(f\"Mean Max: {max_values.mean().item()} | Std Dev: {max_values.std().item()}\")\n",
    "    print(f\"Mean Min: {min_values.mean().item()} | Std Dev: {min_values.std().item()}\")    \n",
    "    \n",
    "#     # # Divide linear model weights by mean_max\n",
    "#     # with torch.no_grad():\n",
    "#     #     linear_nn.fc1.weight.div_(mean_max)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train:\n",
    "    torch.save(linear_nn.state_dict(), f'saved_models/linear_probe_contrastive_c{total_classes}_bs4096_400_400_0.1.pth')\n",
    "    \n",
    "    \n",
    "raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model\n",
    "\n",
    "##### Some qualitative checks on the linear probe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "linear_nn = LinearProbe(hidden_dim, total_classes).to(device)\n",
    "model_path = f'saved_models/linear_probe_contrastive_c{total_classes}.pth'\n",
    "linear_nn.load_state_dict(torch.load(model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_nn.eval()\n",
    "\n",
    "max_values = []\n",
    "min_values = []\n",
    "\n",
    "for i, batch in enumerate(val_dataloader):\n",
    "    X = batch[0].to(device)\n",
    "    y = batch[1].to(device)\n",
    "    with torch.no_grad():\n",
    "        logits = linear_nn(X)\n",
    "    max_values.append(logits.max(dim=1).values)\n",
    "    min_values.append(logits.min(dim=1).values)\n",
    "    \n",
    "max_values = torch.cat(max_values)\n",
    "min_values = torch.cat(min_values)\n",
    "\n",
    "print(f\"Mean Max: {max_values.mean().item()} | Std Dev: {max_values.std().item()}\")\n",
    "print(f\"Mean Min: {min_values.mean().item()} | Std Dev: {min_values.std().item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find three sigma outliers\n",
    "outliers = (max_values > max_values.mean() + 3 * max_values.std()) | (min_values < min_values.mean() - 3 * min_values.std())\n",
    "outliers = outliers.cpu()\n",
    "len(torch.unique(y_val[outliers]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get accuracy on train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_nn.eval()\n",
    "\n",
    "correct = 0\n",
    "# Let's calculate the accuracy on the train set\n",
    "for i, batch in enumerate(train_dataloader):\n",
    "    with torch.no_grad():\n",
    "        X = batch[0].to(device)\n",
    "        y = batch[1].to(device)\n",
    "\n",
    "        outputs = linear_nn(X)\n",
    "        labels = outputs.argmax(dim=1)\n",
    "\n",
    "        correct += (labels == y).sum()\n",
    "\n",
    "print(\"Train Accuracy: \", correct.item()/X_train.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get accuracy on val dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "correct = 0\n",
    "# Let's calculate the accuracy on the train set\n",
    "for i, batch in enumerate(val_dataloader):\n",
    "    with torch.no_grad():\n",
    "        X = batch[0].to(device)\n",
    "        y = batch[1].to(device)\n",
    "\n",
    "        outputs = linear_nn(X)\n",
    "        labels = outputs.argmax(dim=1)\n",
    "\n",
    "        correct += (labels == y).sum()\n",
    "\n",
    "print(\"Validation Accuracy: \", correct.item()/X_val.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top2_values = []\n",
    "for i, batch in enumerate(val_dataloader):\n",
    "    with torch.no_grad():\n",
    "        X = batch[0].to(device)\n",
    "        y = batch[1].to(device)\n",
    "\n",
    "        outputs = linear_nn(X)\n",
    "        # Store diff between top 2 values\n",
    "        topk_values, _ = outputs.topk(2)\n",
    "        top2_values.append(topk_values[:, :2])\n",
    "\n",
    "    \n",
    "top2_values = torch.cat(top2_values).cpu()\n",
    "\n",
    "diff_values = top2_values[:, 0] - top2_values[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_values = top2_values[:, 0]\n",
    "mean = max_values.mean()\n",
    "std_dev = max_values.std()\n",
    "zscores = (max_values - mean) / std_dev\n",
    "\n",
    "print(\"Mean: \", mean.item())\n",
    "print(\"Standard Deviation: \", std_dev.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mean = diff_values.mean()\n",
    "std_dev = diff_values.std()\n",
    "zscores = (diff_values - mean) / std_dev\n",
    "\n",
    "# remove outliers\n",
    "diff_values = diff_values[zscores < 3]\n",
    "\n",
    "# plot histogram of diff values\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(diff_values.numpy(), bins=1000)\n",
    "plt.xlabel('Difference between top 2 logits')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Difference between top 2 logits Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some manual qualititive checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 46\n",
    "X = X_val[idx].unsqueeze(0).to(device)\n",
    "with torch.no_grad():\n",
    "    logits = linear_nn(X)\n",
    "\n",
    "\n",
    "# plots\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(logits.cpu().numpy().flatten(), bins=100)\n",
    "plt.xlabel('Logits')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Logits Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build watermark matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "rng = torch.Generator(device=device)\n",
    "\n",
    "vocab_size = len(tokenizer)\n",
    "gamma = 0.50\n",
    "hash_key = 15485863\n",
    "\n",
    "\n",
    "def prf_lookup(input_ids):\n",
    "    return hash_key * input_ids[-1:].sum().item()\n",
    "\n",
    "\n",
    "def get_partition(input_ids):\n",
    "    prf_key = prf_lookup(input_ids)\n",
    "    rng.manual_seed(prf_key % (2**64 - 1))\n",
    "\n",
    "    greenlist_size = int(vocab_size * gamma)\n",
    "    vocab_permutation = torch.randperm(\n",
    "        vocab_size, device=device, generator=rng)\n",
    "    greenlist_ids = vocab_permutation[:greenlist_size].to(\"cpu\")\n",
    "    redlist_ids = vocab_permutation[greenlist_size:].to(\"cpu\")\n",
    "    return greenlist_ids, redlist_ids\n",
    "\n",
    "# values x keys\n",
    "watermark_matrix = torch.zeros(len(tokenizer), len(tokenizer)).to(device)\n",
    "\n",
    "\n",
    "for i in tqdm(range(len(tokenizer))):\n",
    "    greenlist_ids, redlist_ids = get_partition(torch.tensor([i]))\n",
    "    watermark_matrix[greenlist_ids, i] = 2.0\n",
    "    watermark_matrix[redlist_ids, i] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 9\n",
    "X = X_val[idx].unsqueeze(0).to(device)\n",
    "y = y_val[idx].unsqueeze(0).to(device)\n",
    "l1_criterion = nn.L1Loss(reduce=False)\n",
    "with torch.no_grad():\n",
    "    logits = linear_nn(X).squeeze(0)\n",
    "    ans = torch.matmul(watermark_matrix, logits)\n",
    "    one_hot = torch.zeros_like(logits)\n",
    "    one_hot[y] = 1\n",
    "    gold = torch.matmul(watermark_matrix, one_hot)\n",
    "    # Get average difference between logits and gold\n",
    "    diff = l1_criterion(logits, one_hot)\n",
    "    max_diff_index = torch.argmax(diff)\n",
    "    print(logits[max_diff_index])\n",
    "    print(one_hot[max_diff_index])\n",
    "    print(logits.topk(5))\n",
    "    print(one_hot.topk(5))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "test_text = \"The cat sat on the \"\n",
    "inputs = tokenizer(test_text, return_tensors=\"pt\").to(model.device)\n",
    "with torch.no_grad():\n",
    "    output = model.generate(\n",
    "        **inputs, return_dict_in_generate=True,  max_length=10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = output.sequences[0]\n",
    "tokenizer.decode(seq, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also do the analysis using the watermark matrix (what i mean when i say i want the logits to be close)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "watermark_matrix = torch.load(\"data/watermark_matrix_gamma0.50_simple1_key15485863.pt\")\n",
    "\n",
    "# dummy logits where everything is a small positive value\n",
    "# probe_outputs = torch.ones((10, 50277), dtype=torch.float32)/10\n",
    "probe_outputs = outputs\n",
    "wm_predicted = einops.einsum(probe_outputs.cpu(), watermark_matrix.T, \"b i, i j -> b j\")\n",
    "\n",
    "print(\"Watermark Delta Prediction: \", wm_predicted.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "greenlist_ids, redlist_ids = get_partition(torch.tensor([187]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Mean Green List: \", wm_predicted[0, greenlist_ids].mean().item())\n",
    "print(\"Mean Red List: \", wm_predicted[0, redlist_ids].mean().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4, 3))\n",
    "sns.histplot(wm_predicted[0, greenlist_ids].cpu(), bins=30, kde=True, color=\"green\")\n",
    "# sns.histplot(wm_predicted[0, redlist_ids].cpu(), bins=30, kde=True, color=\"red\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Magnitude of the Logits :- Clean Analysis of the Logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Check how biased these logits are using this loss: f2 + f3 loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4, 3))\n",
    "sns.histplot(wm_predicted[7].cpu(), bins=30, kde=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "greenlist_ids, redlist_ids = get_partition(torch.tensor([y[6]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4, 3))\n",
    "sns.histplot(wm_predicted[6, greenlist_ids].cpu(), bins=30, kde=True, color=\"green\")\n",
    "sns.histplot(wm_predicted[6, redlist_ids].cpu(), bins=30, kde=True, color=\"red\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Check the MSE loss on the watermark logits "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "watermark_matrix = torch.load(\"data/watermark_matrix_gamma0.50_simple1_key15485863.pt\")\n",
    "# watermark_matrix = watermark_matrix.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_loss = nn.MSELoss()\n",
    "linear_nn.eval()\n",
    "\n",
    "total_loss = 0\n",
    "with torch.no_grad():\n",
    "    for i, batch in tqdm(enumerate(val_dataloader)):\n",
    "        X = batch[0]\n",
    "        y = batch[1]\n",
    "\n",
    "        y_onehot = F.one_hot(y, num_classes=len(tokenizer)).to(torch.float32)\n",
    "        y_onehot = y_onehot\n",
    "\n",
    "        outputs = linear_nn(X.to(device))\n",
    "        outputs = torch.sigmoid(outputs).to(\"cpu\")\n",
    "\n",
    "        wm_predicted = einops.einsum(outputs, watermark_matrix.T, \"b i, i j -> b j\")\n",
    "        wm_gt = watermark_matrix[:, y].T\n",
    "\n",
    "        loss = mse_loss(wm_predicted, wm_gt)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        if i >= 100:\n",
    "            break\n",
    "\n",
    "        #? to check if matrix mult is okay\n",
    "        # wm_generated = einops.einsum(y_onehot, watermark_matrix.T, \"b i, i j -> b j\")\n",
    "        # wm_gt = watermark_matrix[:, y].T\n",
    "        # diff = wm_generated - wm_gt\n",
    "        # print(diff.sum())\n",
    "\n",
    "# len(val_dataloader) * val_dataloader.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_val_loss = total_loss/100\n",
    "print(avg_val_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the watermark matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate the watermark matrix and save it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Device: \", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the column i of this matrix should be the green-red split for the column i.\n",
    "rng = torch.Generator(device=device)\n",
    "\n",
    "vocab_size = len(tokenizer)\n",
    "gamma = 0.50\n",
    "hash_key = 15485863\n",
    "\n",
    "def prf_lookup(input_ids):\n",
    "    return hash_key * input_ids[-1:].sum().item()\n",
    "\n",
    "def get_partition(input_ids):\n",
    "    prf_key = prf_lookup(input_ids)\n",
    "    rng.manual_seed(prf_key % (2**64 - 1))\n",
    "\n",
    "    greenlist_size = int(vocab_size * gamma)\n",
    "    vocab_permutation = torch.randperm(vocab_size, device=device, generator=rng)\n",
    "    greenlist_ids = vocab_permutation[:greenlist_size].to(\"cpu\")\n",
    "    redlist_ids = vocab_permutation[greenlist_size:].to(\"cpu\")\n",
    "    return greenlist_ids, redlist_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "watermark_matrix = torch.zeros(len(tokenizer), len(tokenizer))\n",
    "\n",
    "for i in tqdm(range(len(tokenizer))):\n",
    "    greenlist_ids, redlist_ids = get_partition(torch.tensor([i]))\n",
    "    watermark_matrix[greenlist_ids, i] = 1.0\n",
    "    watermark_matrix[redlist_ids, i] = -1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(watermark_matrix, \"data/watermark_matrix_gamma0.50_simple1_key15485863.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the watermark matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "watermark_matrix = torch.load(\"data/watermark_matrix_gamma0.50_simple1_key15485863.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Two Hidden Layer NN with second matrix frozen as the watermark matrix\n",
    "\n",
    "Things tried\n",
    "\n",
    "- Regression loss between the multi-hot encoded vectors and logits (does not work well).\n",
    "- Formulate the problem as multi-label classification problem. `saved as config 2`\n",
    "- Loss as the cosine similarity between the outputs of the model and watermark logits (make it +1 and -1).\n",
    "\n",
    "TODO:\n",
    "\n",
    "- [] Capped ReLU.\n",
    "- [] Sparsity in the intermediate representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WatermarkNetwork(nn.Module):\n",
    "    def __init__(self, hdim=2048, vocab_size=len(tokenizer), watermark_matrix=None):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(hdim, vocab_size)\n",
    "        self.act1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(vocab_size, vocab_size)\n",
    "\n",
    "        # set the watermark matrix as the second layer\n",
    "        self.fc2.weight = nn.Parameter(watermark_matrix, requires_grad=False)\n",
    "        self.fc2.bias = nn.Parameter(torch.zeros(vocab_size), requires_grad=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.act1(self.fc1(x))\n",
    "        return self.fc2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wmNet = WatermarkNetwork(watermark_matrix=watermark_matrix)\n",
    "wmNet.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Will try training the neural network with a regression loss on the watermark outputs\n",
    "batch_size = 64\n",
    "learning_rate = 1e-3\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=32)\n",
    "\n",
    "# freeze the second layer\n",
    "wmNet.fc2.weight.requires_grad = False\n",
    "wmNet.fc2.bias.requires_grad = False\n",
    "\n",
    "# criterion = nn.MSELoss()\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.AdamW(wmNet.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in wmNet.named_parameters():\n",
    "    print(f\"{name}: requires_grad={param.requires_grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the total number of parameters\n",
    "total_params = sum(p.numel() for p in wmNet.parameters() if p.requires_grad)\n",
    "print(\"Total number of trainable parameters:\", total_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 1\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, batch  in tqdm(enumerate(train_dataloader)):\n",
    "        \n",
    "        X = batch[0].to(device)\n",
    "        y = batch[1].to(device)\n",
    "\n",
    "        output = wmNet(X)\n",
    "        targets = watermark_matrix[:, y.to(\"cpu\")]\n",
    "        targets = targets.permute(1, 0).to(device)\n",
    "\n",
    "        loss = criterion(output, targets)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if i % 1000 == 0:\n",
    "            print(f'Step [{i + 1}/{len(train_dataloader)}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(wmNet.state_dict(), 'saved_models/wm_config2.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate some text and (1) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
